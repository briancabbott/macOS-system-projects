This is am-utils.info, produced by makeinfo version 4.1 from
am-utils.texi.

INFO-DIR-SECTION Administration
START-INFO-DIR-ENTRY
* Am-utils: (am-utils).          The Amd automounter suite of utilities
END-INFO-DIR-ENTRY


File: am-utils.info,  Node: FSinfo errors,  Prev: FSinfo Command Line Options,  Up: FSinfo

Errors produced by FSinfo
=========================

   The following table documents the errors and warnings which FSinfo
may produce.

" expected
     Occurs if an unescaped newline is found in a quoted string.

ambiguous mount: VOLUME is a replicated filesystem
     If several filesystems are declared as having the same volume
     name, they will be considered replicated filesystems.  To mount a
     replicated filesystem statically, a specific host will need to be
     named, to say which particular copy to try and mount, else this
     error will result.

can't open FILENAME for writing
     Occurs if any errors are encountered when opening an output file.

cannot determine localname since volname VOLUME is not uniquely defined
     If a volume is replicated and an attempt is made to mount the
     filesystem statically without specifying a local mountpoint,
     FSinfo cannot calculate a mountpoint, as the desired pathname
     would be ambiguous.

DEVICE has duplicate exportfs data
     Produced if the `exportfs' option is used multiple times within the
     same branch of a filesystem definition. For example, if you
     attempt to set the `exportfs' data at different levels of the
     mountpoint directory tree.

dump frequency for HOST:DEVICE is non-zero
     Occurs if DEVICE has its `fstype' declared to be `swap' or
     `export' and the `dump' option is set to a value greater than
     zero.  Swap devices should not be dumped.

duplicate host HOSTNAME!
     If a host has more than one definition.

end of file within comment
     A comment was unterminated before the end of one of the
     configuration files.

FILENAME: cannot open for reading
     If a file specified on the command line as containing
     configuration data could not be opened.

FILESYSTEM has a volname but no exportfs data
     Occurs when a volume name is declared for a file system, but the
     string specifying what machines the filesystem can be exported to
     is missing.

fs field "FIELD-NAME" already set
     Occurs when multiple definitions are given for one of the
     attributes of a host's filesystem.

host field "FIELD-NAME" already set
     If duplicate definitions are given for any of the fields with a
     host definition.

HOST:DEVICE has more than one mount point
     Occurs if the mount option for a host's filesystem specifies
     multiple trees at which to place the mountpoint.

HOST:DEVICE has no mount point
     Occurs if the `mount' option is not specified for a host's
     filesystem.

HOST:DEVICE needs field "FIELD-NAME"
     Occurs when a filesystem is missing a required field. FIELD-NAME
     could be one of `fstype', `opts', `passno' or `mount'.

HOST:mount field specified for swap partition
     Occurs if a mountpoint is given for a filesystem whose type is
     declared to be `swap'.

malformed IP dotted quad: ADDRESS
     If the Internet address of an interface is incorrectly specified.
     An Internet address definition is handled to inet_addr(3N) to see
     if it can cope.  If not, then this message will be displayed.

malformed netmask: NETMASK
     If the netmask cannot be decoded as though it were a hexadecimal
     number, then this message will be displayed.  It will typically be
     caused by incorrect characters in the NETMASK value.

mount field "FIELD-NAME" already set
     Occurs when a static mount has multiple definitions of the same
     field.

mount tree field "FIELD-NAME" already set
     Occurs when the FIELD-NAME is defined more than once during the
     definition of a filesystems mountpoint.

netif field FIELD-NAME already set
     Occurs if you attempt to define an attribute of an interface more
     than once.

network booting requires both root and swap areas
     Occurs if a machine has mount declarations for either the root
     partition or the swap area, but not both.  You cannot define a
     machine to only partially boot via the network.

no disk mounts on HOSTNAME
     If there are no static mounts, nor local disk mounts specified for
     a machine, this message will be displayed.

no volname given for HOST:DEVICE
     Occurs when a filesystem is defined to be mounted on `default', but
     no volume name is given for the file system, then the mountpoint
     cannot be determined.

not allowed '/' in a directory name
     Occurs when a pathname with multiple directory elements is
     specified as the name for an automounter tree.  A tree should only
     have one name at each level.

pass number for HOST:DEVICE is non-zero
     Occurs if DEVICE has its `fstype' declared to be `swap' or
     `export' and the fsck(8) pass number is set. Swap devices should
     not be fsck'd.  *Note FSinfo fstype Option::.

sub-directory DIRECTORY of DIRECTORY-TREE starts with '/'
     Within the filesystem specification for a host, if an element
     DIRECTORY of the mountpoint begins with a `/' and it is not the
     start of the tree.

sub-directory of DIRECTORY-TREE is named "default"
     `default' is a keyword used to specify if a mountpoint should be
     automatically calculated by FSinfo.  If you attempt to specify a
     directory name as this, it will use the filename of `default' but
     will produce this warning.

unknown \ sequence
     Occurs if an unknown escape sequence is found inside a string.
     Within a string, you can give the standard C escape sequences for
     strings, such as newlines and tab characters.

unknown directory attribute
     If an unknown keyword is found while reading the definition of a
     host's filesystem mount option.

unknown filesystem attribute
     Occurs if an unrecognized keyword is used when defining a host's
     filesystems.

unknown host attribute
     Occurs if an unrecognized keyword is used when defining a host.

unknown mount attribute
     Occurs if an unrecognized keyword is found while parsing the list
     of static mounts.

unknown volname VOLUME automounted [ on name ]
     Occurs if VOLUME is used in a definition of an automount map but
     the volume name has not been declared during the host filesystem
     definitions.

volname VOLUME is unknown
     Occurs if an attempt is made to mount or reference a volume name
     which has not been declared during the host filesystem definitions.

volname VOLUME not exported from MACHINE
     Occurs if you attempt to mount the volume VOLUME from a machine
     which has not declared itself to have such a filesystem available.


File: am-utils.info,  Node: Hlfsd,  Next: Assorted Tools,  Prev: FSinfo,  Up: Top

Hlfsd
*****

   Hlfsd is a daemon which implements a filesystem containing a
symbolic link to subdirectory within a user's home directory, depending
on the user which accessed that link.  It was primarily designed to
redirect incoming mail to users' home directories, so that it can be
read from anywhere.  It was designed and implemented by Erez Zadok
<ezk@cs.columbia.edu> and Alexander Dupuy <dupuy@cs.columbia.edu>, at
the Computer Science Department (http://www.cs.columbia.edu/) of
Columbia University (http://www.columbia.edu/).  A paper
(http://www.cs.columbia.edu/~ezk/research/hlfsd/hlfsd.html) on Hlfsd
was presented at the Usenix LISA VII conference in 1993.

   Hlfsd operates by mounting itself as an NFS server for the directory
containing linkname, which defaults to `/hlfs/home'.  Lookups within
that directory are handled by Hlfsd, which uses the password map to
determine how to resolve the lookup.  The directory will be created if
it doesn't already exist.  The symbolic link will be to the accessing
user's home directory, with subdir appended to it.  If not specified,
subdir defaults to `.hlfsdir'.  This directory will also be created if
it does not already exist.

   A `SIGTERM' sent to Hlfsd will cause it to shutdown.  A `SIGHUP'
will flush the internal caches, and reload the password map.  It will
also close and reopen the log file, to enable the original log file to
be removed or rotated.  A `SIGUSR1' will cause it to dump its internal
table of user IDs and home directories to the file `/tmp/hlfsddump'.

* Menu:

* Introduction to Hlfsd::
* Background to Mail Delivery::
* Using Hlfsd::


File: am-utils.info,  Node: Introduction to Hlfsd,  Next: Background to Mail Delivery,  Prev: Hlfsd,  Up: Hlfsd

Introduction to Hlfsd
=====================

   Electronic mail has become one of the major applications for many
computer networks, and use of this service is expected to increase over
time, as networks proliferate and become faster.  Providing a convenient
environment for users to read, compose, and send electronic mail has
become a requirement for systems administrators (SAs).

   Widely used methods for handling mail usually require users to be
logged into a designated "home" machine, where their mailbox files
reside.  Only on that one machine can they read newly arrived mail.
Since users have to be logged into that system to read their mail, they
often find it convenient to run all of their other processes on that
system as well, including memory and CPU-intensive jobs.  For example,
in our department, we have allocated and configured several
multi-processor servers to handle such demanding CPU/memory
applications, but these were underutilized, in large part due to the
inconvenience of not being able to read mail on those machines.  (No
home directories were located on these designated CPU-servers, since we
did not want NFS service for users' home directories to have to compete
with CPU-intensive jobs.  At the same time, we discouraged users from
running demanding applications on their home machines.)

   Many different solutions have been proposed to allow users to read
their mail on any host.  However, all of these solutions fail in one or
more of several ways:

   * they introduce new single points of failure

   * they require using different mail transfer agents (MTAs) or user
     agents (UAs)

   * they do not solve the problem for all cases, i.e.  the solution is
     only partially successful for a particular environment.


   We have designed a simple filesystem, called the "Home-Link File
System", to provide the ability to deliver mail to users' home
directories, without modification to mail-related applications. We have
endeavored to make it as stable as possible.  Of great importance to us
was to make sure the HLFS daemon, `hlfsd' , would not hang under any
circumstances, and would take the next-best action when faced with
problems.  Compared to alternative methods, Hlfsd is a stable, more
general solution, and easier to install/use.  In fact, in some ways, we
have even managed to improve the reliability and security of mail
service.

   Our server implements a small filesystem containing a symbolic link
to a subdirectory of the invoking user's home directory, and named
symbolic links to users' mailbox files.

   The Hlfsd server finds out the UID of the process that is accessing
its mount point, and resolves the pathname component `home' as a
symbolic link to a subdirectory within the home directory given by the
UID's entry in the password file.  If the GID of the process that
attempts to access a mailbox file is a special one (called HLFS_GID),
then the server maps the name of the _next_ pathname component directly
to the user's mailbox.  This is necessary so that access to a mailbox
file by users other than the owner can succeed.  The server has safety
features in case of failures such as hung filesystems or home directory
filesystems that are inaccessible or full.

   On most of our machines, mail gets delivered to the directory
`/var/spool/mail'. Many programs, including UAs, depend on that path.
Hlfsd creates a directory `/mail', and mounts itself on top of that
directory.  Hlfsd implements the path name component called `home',
pointing to a subdirectory of the user's home directory.  We have made
`/var/spool/mail' a symbolic link to `/mail/home', so that accessing
`/var/spool/mail' actually causes access to a subdirectory within a
user's home directory.

   The following table shows an example of how resolving the pathname
`/var/mail/NAME' to `/users/ezk/.mailspool/NAME' proceeds.

Resolving Component   Pathname left to resolve   Value if symbolic link
/                     var/mail/NAME              
var/                  mail/NAME                  
mail@                 /mail/home/NAME            mail@ -> /mail/home
/                     mail/home/NAME             
mail/                 home/NAME                  
home@                 NAME                       home@ ->
                                                 /users/ezk/.mailspool
/                     users/ezk/.mailspool/NAME  
users/                ezk/.mailspool/NAME        
ezk/                  .mailspool/NAME            
.mailspool/           NAME                       
NAME                                             


File: am-utils.info,  Node: Background to Mail Delivery,  Next: Using Hlfsd,  Prev: Introduction to Hlfsd,  Up: Hlfsd

Background to Mail Delivery
===========================

   This section provides an in-depth discussion of why available methods
for delivering mail to home directories are not as good as the one used
by Hlfsd.

* Menu:

* Single-Host Mail Spool Directory::
* Centralized Mail Spool Directory::
* Distributed Mail Spool Service::
* Why Deliver Into the Home Directory?::


File: am-utils.info,  Node: Single-Host Mail Spool Directory,  Next: Centralized Mail Spool Directory,  Prev: Background to Mail Delivery,  Up: Background to Mail Delivery

Single-Host Mail Spool Directory
--------------------------------

   The most common method for mail delivery is for mail to be appended
to a mailbox file in a standard spool directory on the designated "mail
home" machine of the user. The greatest advantage of this method is
that it is the default method most vendors provide with their systems,
thus very little (if any) configuration is required on the SA's part.
All they need to set up are mail aliases directing mail to the host on
which the user's mailbox file is assigned.  (Otherwise, mail is
delivered locally, and users find mailboxes on many machines.)

   As users become more sophisticated, and aided by windowing systems,
they find themselves logging in on multiple hosts at once, performing
several tasks concurrently.  They ask to be able to read their mail on
any host on the network, not just the one designated as their "mail
home".


File: am-utils.info,  Node: Centralized Mail Spool Directory,  Next: Distributed Mail Spool Service,  Prev: Single-Host Mail Spool Directory,  Up: Background to Mail Delivery

Centralized Mail Spool Directory
--------------------------------

   A popular method for providing mail readability from any host is to
have all mail delivered to a mail spool directory on a designated
"mail-server" which is exported via NFS to all of the hosts on the
network.  Configuring such a system is relatively easy.  On most
systems, the bulk of the work is a one-time addition to one or two
configuration files in `/etc'.  The file-server's spool directory is
then hard-mounted across every machine on the local network.  In small
environments with only a handful of hosts this can be an acceptable
solution.  In our department, with a couple of hundred active hosts and
thousands of mail messages processed daily, this was deemed completely
unacceptable, as it introduced several types of problems:

Scalability and Performance
     As more and more machines get added to the network, more mail
     traffic has to go over NFS to and from the mail-server. Users like
     to run mail-watchers, and read their mail often. The stress on the
     shared infrastructure increases with every user and host added;
     loads on the mail server would most certainly be high since all
     mail delivery goes through that one machine.(1)  This leads to
     lower reliability and performance.  To reduce the number of
     concurrent connections between clients and the server host, some
     SAs have resorted to automounting the mail-spool directory.  But
     this solution only makes things worse: since users often run mail
     watchers, and many popular applications such as `trn', `emacs',
     `csh' or `ksh' check periodically for new mail, the automounted
     directory would be effectively permanently mounted.  If it gets
     unmounted automatically by the automounter program, it is most
     likely to get mounted shortly afterwards, consuming more I/O
     resources by the constant cycle of mount and umount calls.

Reliability
     The mail-server host and its network connectivity must be very
     reliable.  Worse, since the spool directory has to be
     hard-mounted,(2) many processes which access the spool directory
     (various shells, `login', `emacs', etc.)  would be hung as long as
     connectivity to the mail-server is severed. To improve
     reliability, SAs may choose to backup the mail-server's spool
     partition several times a day.  This may make things worse since
     reading or delivering mail while backups are in progress may cause
     backups to be inconsistent; more backups consume more backup-media
     resources, and increase the load on the mail-server host.

   ---------- Footnotes ----------

   (1)  Delivery via NFS-mounted filesystems may require usage of
`rpc.lockd' and `rpc.statd' to provide distributed file-locking, both
of which are widely regarded as unstable and unreliable.  Furthermore,
this will degrade performance, as local processes as well as remote
`nfsd' processes are kept busy.

   (2) No SA in their right minds would soft-mount read/write
partitions -- the chances for data loss are too great.


File: am-utils.info,  Node: Distributed Mail Spool Service,  Next: Why Deliver Into the Home Directory?,  Prev: Centralized Mail Spool Directory,  Up: Background to Mail Delivery

Distributed Mail Spool Service
------------------------------

   Despite the existence of a few systems that support delivery to
users' home directories, mail delivery to home directories hasn't
caught on.  We believe the main reason is that there are too many
programs that "know" where mailbox files reside.  Besides the obvious
(the delivery program `/bin/mail' and mail readers like `/usr/ucb/Mail',
`mush', `mm', etc.), other programs that know mailbox location are
login, from, almost every shell, `xbiff', `xmailbox', and even some
programs not directly related to mail, such as `emacs' and `trn'.
Although some of these programs can be configured to look in different
directories with the use of environment variables and other resources,
many of them cannot.  The overall porting work is significant.

   Other methods that have yet to catch on require the use of a special
mail-reading server, such as IMAP or POP.  The main disadvantage of
these systems is that UAs need to be modified to use these services --
a long and involved task.  That is why they are not popular at this
time.

   Several other ideas have been proposed and even used in various
environments.  None of them is robust.  They are mostly very
specialized, inflexible, and do not extend to the general case.  Some of
the ideas are plain bad, potentially leading to lost or corrupt mail:

automounters
     Using an automounter such as Amd to provide a set of symbolic links
     from the normal spool directory to user home directories is not
     sufficient.  UAs rename, unlink, and recreate the mailbox as a
     regular file, therefore it must be a real file, not a symbolic
     link.  Furthermore, it must reside in a real directory which is
     writable by the UAs and MTAs.  This method may also require
     populating `/var/spool/mail' with symbolic links and making sure
     they are updated.  Making Amd manage that directory directly
     fails, since many various lock files need to be managed as well.
     Also, Amd does not provide all of the NFS operations which are
     required to write mail such as write, create, remove, and unlink.

`$MAIL'
     Setting this variable to an automounted directory pointing to the
     user's mail spool host only solves the problem for those programs
     which know and use `$MAIL'.  Many programs don't, therefore this
     solution is partial and of limited flexibility.  Also, it requires
     the SAs or the users to set it themselves -- an added level of
     inconvenience and possible failures.

/bin/mail
     Using a different mail delivery agent could be the solution.  One
     such example is `hdmail'.  However, `hdmail' still requires
     modifying all UAs, the MTA's configuration, installing new
     daemons, and changing login scripts.  This makes the system less
     upgradable or compatible with others, and adds one more
     complicated system for SAs to deal with.  It is not a complete
     solution because it still requires each user have their `$MAIL'
     variable setup correctly, and that every program use this variable.


File: am-utils.info,  Node: Why Deliver Into the Home Directory?,  Prev: Distributed Mail Spool Service,  Up: Background to Mail Delivery

Why Deliver Into the Home Directory?
------------------------------------

   There are several major reasons why SAs might want to deliver mail
directly into the users' home directories:

Location
     Many mail readers need to move mail from the spool directory to the
     user's home directory.  It speeds up this operation if the two are
     on the same filesystem.  If for some reason the user's home
     directory is inaccessible, it isn't that useful to be able to read
     mail, since there is no place to move it to.  In some cases,
     trying to move mail to a non-existent or hung filesystem may
     result in mail loss.

Distribution
     Having all mail spool directories spread among the many more
     filesystems minimizes the chances that complete environments will
     grind to a halt when a single server is down.  It does increase
     the chance that there will be someone who is not able to read
     their mail when a machine is down, but that is usually preferred
     to having no one be able to read their mail because a centralized
     mail server is down.  The problem of losing some mail due to the
     (presumably) higher chances that a user's machine is down is
     minimized in HLFS.

Security
     Delivering mail to users' home directories has another advantage --
     enhanced security and privacy.  Since a shared system mail spool
     directory has to be world-readable and searchable, any user can see
     whether other users have mail, when they last received new mail,
     or when they last read their mail.  Programs such as `finger'
     display this information, which some consider an infringement of
     privacy.  While it is possible to disable this feature of `finger'
     so that remote users cannot see a mailbox file's status, this
     doesn't prevent local users from getting the information.
     Furthermore, there are more programs which make use of this
     information.  In shared environments, disabling such programs has
     to be done on a system-wide basis, but with mail delivered to
     users' home directories, users less concerned with privacy who do
     want to let others know when they last received or read mail can
     easily do so using file protection bits.

   In summary, delivering mail to home directories provides users the
functionality sought, and also avoids most of the problems just
discussed.


File: am-utils.info,  Node: Using Hlfsd,  Prev: Background to Mail Delivery,  Up: Hlfsd

Using Hlfsd
===========

* Menu:

* Controlling Hlfsd::
* Hlfsd Options::
* Hlfsd Files::


File: am-utils.info,  Node: Controlling Hlfsd,  Next: Hlfsd Options,  Prev: Using Hlfsd,  Up: Using Hlfsd

Controlling Hlfsd
-----------------

   Much the same way Amd is controlled by `ctl-amd', so does Hlfsd get
controlled by the `ctl-hlfsd' script:

ctl-hlfsd start
     Start a new Hlfsd.

ctl-hlfsd stop
     Stop a running Hlfsd.

ctl-hlfsd restart
     Stop a running Hlfsd, wait for 10 seconds, and then start a new
     one.  It is hoped that within 10 seconds, the previously running
     Hlfsd terminate properly; otherwise, starting a second one could
     cause system lockup.

   For example, on our systems, we start Hlfsd within `ctl-hlfsd' as
follows on Solaris 2 systems:

     hlfsd -a /var/alt_mail -x all -l /var/log/hlfsd /mail/home .mailspool

   The directory `/var/alt_mail' is a directory in the root partition
where alternate mail will be delivered into, when it cannot be delivered
into the user's home directory.

   Normal mail gets delivered into `/var/mail', but on our systems,
that is a symbolic link to `/mail/home'.  `/mail' is managed by Hlfsd,
which creates a dynamic symlink named `home', pointing to the
subdirectory `.mailspool' _within_ the accessing user's home directory.
This results in mail which normally should go to `/var/mail/`$USER'',
to go to ``$HOME'/.mailspool/`$USER''.

   Hlfsd does not create the `/var/mail' symlink.  This needs to be
created (manually) once on each host, by the system administrators, as
follows:

     mv /var/mail /var/alt_mail
     ln -s /mail/home /var/mail


File: am-utils.info,  Node: Hlfsd Options,  Next: Hlfsd Files,  Prev: Controlling Hlfsd,  Up: Using Hlfsd

Hlfsd Options
-------------

-a ALT_DIR
     Alternate directory.  The name of the directory to which the
     symbolic link returned by Hlfsd will point, if it cannot access
     the home directory of the user.  This defaults to `/var/hlfs'.
     This directory will be created if it doesn't exist.  It is
     expected that either users will read these files, or the system
     administrators will run a script to resend this "lost mail" to its
     owner.

-c CACHE-INTERVAL
     Caching interval.  Hlfsd will cache the validity of home
     directories for this interval, in seconds.  Entries which have
     been verified within the last CACHE-INTERVAL seconds will not be
     verified again, since the operation could be expensive, and the
     entries are most likely still valid.  After the interval has
     expired, Hlfsd will re-verify the validity of the user's home
     directory, and reset the cache time-counter.  The default value
     for CACHE-INTERVAL is 300 seconds (5 minutes).

-f
     Force fast startup.  This option tells Hlfsd to skip startup-time
     consistency checks such as existence of mount directory, alternate
     spool directory, symlink to be hidden under the mount directory,
     their permissions and validity.

-g GROUP
     Set the special group HLFS_GID to GROUP.  Programs such as
     `/usr/ucb/from' or `/usr/sbin/in.comsat', which access the
     mailboxes of other users, must be setgid `HLFS_GID' to work
     properly.  The default group is `hlfs'.  If no group is provided,
     and there is no group `hlfs', this feature is disabled.

-h
     Help.  Print a brief help message, and exit.

-i RELOAD-INTERVAL
     Map-reloading interval.  Each RELOAD-INTERVAL seconds, Hlfsd will
     reload the password map.  Hlfsd needs the password map for the
     UIDs and home directory pathnames.  Hlfsd schedules a `SIGALRM' to
     reload the password maps.  A `SIGHUP' sent to Hlfsd will force it
     to reload the maps immediately.  The default value for
     RELOAD-INTERVAL is 900 seconds (15 minutes.)

-l LOGFILE
     Specify a log file to which Hlfsd will record events.  If LOGFILE
     is the string `syslog' then the log messages will be sent to the
     system log daemon by syslog(3), using the `LOG_DAEMON' facility.
     This is also the default.

-n
     No verify.  Hlfsd will not verify the validity of the symbolic link
     it will be returning, or that the user's home directory contains
     sufficient disk-space for spooling.  This can speed up Hlfsd at the
     cost of possibly returning symbolic links to home directories
     which are not currently accessible or are full.  By default, Hlfsd
     validates the symbolic-link in the background.  The `-n' option
     overrides the meaning of the `-c' option, since no caching is
     necessary.

-o MOUNT-OPTIONS
     Mount options which Hlfsd will use to mount itself on top of
     DIRNAME.  By default, MOUNT-OPTIONS is set to `ro'.  If the system
     supports symbolic-link caching, default options are set to
     `ro,nocache'.

-p
     Print PID.  Outputs the process-id of Hlfsd to standard output
     where it can be saved into a file.

-v
     Version.  Displays version information to standard error.

-x LOG-OPTIONS
     Specify run-time logging options.  The options are a comma
     separated list chosen from: `fatal', `error', `user', `warn',
     `info', `map', `stats', `all'.

-C
     Force Hlfsd to run on systems that cannot turn off the NFS
     attribute-cache.  Use of this option on those systems is
     discouraged, as it may result in loss or misdelivery of mail.  The
     option is ignored on systems that can turn off the attribute-cache.

-D LOG-OPTIONS
     Select from a variety of debugging options.  Prefixing an option
     with the string `no' reverses the effect of that option.  Options
     are cumulative.  The most useful option is `all'.  Since this
     option is only used for debugging other options are not documented
     here.  A fuller description is available in the program source.  A
     `SIGUSR1' sent to Hlfsd will cause it to dump its internal
     password map to the file `/usr/tmp/hlfsd.dump.XXXXXX', where
     `XXXXXX' will be replaced by a random string generated by
     mktemp(3) or (the more secure) mkstemp(3).

-P PASSWORD-FILE
     Read the user-name, user-id, and home directory information from
     the file PASSWORD-FILE.  Normally, Hlfsd will use getpwent(3) to
     read the password database.  This option allows you to override the
     default database, and is useful if you want to map users' mail
     files to a directory other than their home directory.  Only the
     username, uid, and home-directory fields of the file PASSWORD-FILE
     are read and checked.  All other fields are ignored.  The file
     PASSWORD-FILE must otherwise be compliant with Unix Version 7
     colon-delimited format passwd(4).


File: am-utils.info,  Node: Hlfsd Files,  Prev: Hlfsd Options,  Up: Using Hlfsd

Hlfsd Files
-----------

   The following files are used by Hlfsd:

`/hlfs'
     directory under which Hlfsd mounts itself and manages the symbolic
     link `home'.

`.hlfsdir'
     default sub-directory in the user's home directory, to which the
     `home' symbolic link returned by Hlfsd points.

`/var/hlfs'
     directory to which `home' symbolic link returned by Hlfsd points
     if it is unable to verify the that user's home directory is
     accessible.

   For discussion on other files used by Hlfsd, see *Note
lostaltmail::, and *Note lostaltmail.conf-sample::.


File: am-utils.info,  Node: Assorted Tools,  Next: Examples,  Prev: Hlfsd,  Up: Top

Assorted Tools
**************

   The following are additional utilities and scripts included with
am-utils, and get installed.

* Menu:

* am-eject::
* amd.conf-sample::
* amd2ldif::
* amd2sun::
* automount2amd::
* ctl-amd::
* ctl-hlfsd::
* expn::
* fix-amd-map::
* fixmount::
* fixrmtab::
* lostaltmail::
* lostaltmail.conf-sample::
* mk-amd-map::
* pawd::
* redhat-ctl-amd::
* wait4amd::
* wait4amd2die::
* wire-test::


File: am-utils.info,  Node: am-eject,  Next: amd.conf-sample,  Prev: Assorted Tools,  Up: Assorted Tools

am-eject
========

   A shell script unmounts a floppy or CD-ROM that is automounted, and
then attempts to eject the removable device.


File: am-utils.info,  Node: amd.conf-sample,  Next: amd2ldif,  Prev: am-eject,  Up: Assorted Tools

amd.conf-sample
===============

   A sample Amd configuration file. *Note Amd Configuration File::.


File: am-utils.info,  Node: amd2ldif,  Next: amd2sun,  Prev: amd.conf-sample,  Up: Assorted Tools

amd2ldif
========

   A script to convert Amd maps to LDAP input files.  Use it as follows:

     amd2ldif mapname base < amd.mapfile > mapfile.ldif


File: am-utils.info,  Node: amd2sun,  Next: automount2amd,  Prev: amd2ldif,  Up: Assorted Tools

amd2sun
=======

   A script to convert Amd maps to Sun Automounter maps.  Use it as
follows

     amd2sun < amd.mapfile > auto_mapfile


File: am-utils.info,  Node: automount2amd,  Next: ctl-amd,  Prev: amd2sun,  Up: Assorted Tools

automount2amd
=============

   A script to convert old Sun Automounter maps to Amd maps.

   Say you have the Sun automount file auto.foo, with these two lines:
     home                  earth:/home
     moon  -ro,intr        server:/proj/images
   Running
     automount2amd auto.foo > amd.foo

   will produce the Amd map amd.foo with this content:

     # generated by automount2amd on Sat Aug 14 17:59:32 US/Eastern 1999
     
     /defaults \\
       type:=nfs;opts:=rw,grpid,nosuid,utimeout=600
     
     home \
       host==earth;type:=link;fs:=/home \\
       rhost:=earth;rfs:=/home
     
     moon \
       -addopts:=ro,intr \\
       host==server;type:=link;fs:=/proj/images \\
       rhost:=server;rfs:=/proj/images

   This perl script will use the following /default entry
     type:=nfs;opts:=rw,grpid,nosuid,utimeout=600
   If you wish to override that, define the $DEFAULTS environment
variable, or modify the script.

   If you wish to generate Amd maps using the hostd (*note hostd
Selector Variable::) Amd map syntax, then define the environment
variable $DOMAIN or modify the script.

   Note that automount2amd does not understand the syntax in newer Sun
Automount maps, those used with autofs.


File: am-utils.info,  Node: ctl-amd,  Next: ctl-hlfsd,  Prev: automount2amd,  Up: Assorted Tools

ctl-amd
=======

   A script to start, stop, or restart Amd.  Use it as follows:

ctl-amd start
     Start a new Amd process.

ctl-amd stop
     Stop the running Amd.

ctl-amd restart
     Stop the running Amd (if any), safely wait for it to terminate, and
     then start a new process -- only if the previous one died cleanly.

   *Note Run-time Administration::, for more details.


File: am-utils.info,  Node: ctl-hlfsd,  Next: expn,  Prev: ctl-amd,  Up: Assorted Tools

ctl-hlfsd
=========

   A script for controlling Hlfsd, much the same way `ctl-amd' controls
Amd.  Use it as follows:

ctl-hlfsd start
     Start a new Hlfsd process.

ctl-hlfsd stop
     Stop the running Hlfsd.

ctl-hlfsd restart
     Stop the running Hlfsd (if any), wait for 10 seconds for it to
     terminate, and then start a new process -- only if the previous one
     died cleanly.

   *Note Hlfsd::, for more details.


File: am-utils.info,  Node: expn,  Next: fix-amd-map,  Prev: ctl-hlfsd,  Up: Assorted Tools

expn
====

   A script to expand email addresses into their full name.  It is
generally useful when using with the `lostaltmail' script, but is a
useful tools otherwise.

     $ expn -v ezk@cs.columbia.edu
     ezk@cs.columbia.edu ->
             ezk@shekel.mcl.cs.columbia.edu
     ezk@shekel.mcl.cs.columbia.edu ->
             Erez Zadok <"| /usr/local/mh/lib/slocal -user ezk || exit 75>
             Erez Zadok <\ezk>
             Erez Zadok </u/zing/ezk/.mailspool/backup>


File: am-utils.info,  Node: fix-amd-map,  Next: fixmount,  Prev: expn,  Up: Assorted Tools

fix-amd-map
===========

   Am-utils changed some of the syntax and default values of some
variables.  For example, the default value for `${os}' for Solaris 2.x
(aka SunOS 5.x) systems used to be `sos5', it is now more automatically
generated from `config.guess' and its value is `sunos5'.

   This script converts older Amd maps to new ones.  Use it as follows:

     fix-amd-map < old.map > new.map


File: am-utils.info,  Node: fixmount,  Next: fixrmtab,  Prev: fix-amd-map,  Up: Assorted Tools

fixmount
========

   `fixmount' is a variant of showmount(8) that can delete bogus mount
entries in remote mountd(8) daemons.  This is useful to cleanup
otherwise ever-accumulating "junk".  Use it for example:

     fixmount -r host

   See the online manual page for `fixmount' for more details of its
usage.


File: am-utils.info,  Node: fixrmtab,  Next: lostaltmail,  Prev: fixmount,  Up: Assorted Tools

fixrmtab
========

   A script to invalidate `/etc/rmtab' entries for hosts named.  Also
restart mountd for changes to take effect.  Use it for example:

     fixrmtab host1 host2 ...


File: am-utils.info,  Node: lostaltmail,  Next: lostaltmail.conf-sample,  Prev: fixrmtab,  Up: Assorted Tools

lostaltmail
===========

   A script used with Hlfsd to resend any "lost" mail.  Hlfsd redirects
mail which cannot be written into the user's home directory to an
alternate directory.  This is useful to continue delivering mail, even
if the user's file system was unavailable, full, or over quota.  But,
the mail which gets delivered to  the alternate directory needs to be
resent to its respective users.  This is what the `lostaltmail' script
does.

   Use it as follows:

     lostaltmail

   This script needs a configuration file `lostaltmail.conf' set up
with the right parameters to properly work.  *Note Hlfsd::, for more
details.


File: am-utils.info,  Node: lostaltmail.conf-sample,  Next: mk-amd-map,  Prev: lostaltmail,  Up: Assorted Tools

lostaltmail.conf-sample
=======================

   This is a text file with configuration parameters needed for the
`lostaltmail' script.  The script includes comments explaining each of
the configuration variables.  See it for more information.  Also *note
Hlfsd:: for general information.


File: am-utils.info,  Node: mk-amd-map,  Next: pawd,  Prev: lostaltmail.conf-sample,  Up: Assorted Tools

mk-amd-map
==========

   This program converts a normal Amd map file into an ndbm database
with the same prefix as the named file.  Use it as follows:

     mk-amd-map mapname


File: am-utils.info,  Node: pawd,  Next: redhat-ctl-amd,  Prev: mk-amd-map,  Up: Assorted Tools

pawd
====

   Pawd is used to print the current working directory, adjusted to
reflect proper paths that can be reused to go through the automounter
for the shortest possible path.  In particular, the path printed back
does not include any of Amd's local mount points.  Using them is
unsafe, because Amd may unmount managed file systems from the mount
points, and thus including them in paths may not always find the files
within.

   Without any arguments, Pawd will print the automounter adjusted
current working directory.  With any number of arguments, it will print
the adjusted path of each one of the arguments.


File: am-utils.info,  Node: redhat-ctl-amd,  Next: wait4amd,  Prev: pawd,  Up: Assorted Tools

redhat-ctl-amd
==============

   This script is similar to ctl-amd (*note ctl-amd::) but is intended
for Red Hat Linux systems.  You can safely copy redhat-ctl-amd onto
`/etc/rc.d/init.d/amd'.  The script supplied by Am-utils is usually
better than the one provided by Red Hat, because the Red Hat script
does not correctly kill Amd processes: it is too quick to kill the
wrong processes, leaving stale or hung mount points behind.


File: am-utils.info,  Node: wait4amd,  Next: wait4amd2die,  Prev: redhat-ctl-amd,  Up: Assorted Tools

wait4amd
========

   A script to wait for Amd to start on a particular host before
performing an arbitrary command.  The command is executed repeatedly,
with 1 second intervals in between.  You may interrupt the script using
`^C' (or whatever keyboard sequence your terminal's `intr' function is
bound to).

   Examples:

wait4amd saturn amq -p -h saturn
     When Amd is up on host `saturn', get the process ID of that
     running Amd.

wait4amd pluto rlogin pluto
     Remote login to host `pluto' when Amd is up on that host.  It is
     generally necessary to wait for Amd to properly start and
     initialize on a remote host before logging in to it, because
     otherwise user home directories may not be accessible across the
     network.

wait4amd pluto
     A short-hand version of the previous command, since the most useful
     reason for this script is to login to a remote host.  I use it very
     often when testing out new versions of Amd, and need to reboot hung
     hosts.


File: am-utils.info,  Node: wait4amd2die,  Next: wire-test,  Prev: wait4amd,  Up: Assorted Tools

wait4amd2die
============

   This script is used internally by `ctl-amd' when used to restart
Amd.  It waits for Amd to terminate.  If it detected that Amd
terminated cleanly, this script will return an exist status of zero.
Otherwise, it will return a non-zero exit status.

   The script tests for Amd's existence once every 5 seconds, six
times, for a total of 30 seconds.  It will return a zero exist status as
soon as it detects that Amd dies.


File: am-utils.info,  Node: wire-test,  Prev: wait4amd2die,  Up: Assorted Tools

wire-test
=========

   A simple program to test if some of the most basic networking
functions in am-util's library `libamu' work.  It also tests the
combination of NFS protocol and version number that are supported from
the current host, to a remote one.

   For example, in this test a machine which only supports NFS Version
2 is contacting a remote host that can support the same version, but
using both UDP and TCP.  If no host name is specified, `wire-test' will
try `localhost'.

     $ wire-test moisil
     Network name is "mcl-lab-net.cs.columbia.edu"
     Network number is "128.59.13"
     Network name is "old-net.cs.columbia.edu"
     Network number is "128.59.16"
     My IP address is 0x7f000001.
     NFS Version and protocol tests to host "moisil"...
             testing vers=2, proto="udp" -> found version 2.
             testing vers=3, proto="udp" -> failed!
             testing vers=2, proto="tcp" -> found version 2.
             testing vers=3, proto="tcp" -> failed!


File: am-utils.info,  Node: Examples,  Next: Internals,  Prev: Assorted Tools,  Up: Top

Examples
********

* Menu:

* User Filesystems::
* Home Directories::
* Architecture Sharing::
* Wildcard Names::
* rwho servers::
* /vol::
* /defaults with selectors::
* /tftpboot in a chroot-ed environment::


File: am-utils.info,  Node: User Filesystems,  Next: Home Directories,  Prev: Examples,  Up: Examples

User Filesystems
================

   With more than one fileserver, the directories most frequently
cross-mounted are those containing user home directories.  A common
convention used at Imperial College is to mount the user disks under
/home/machine.

   Typically, the `/etc/fstab' file contained a long list of entries
such as:

     machine:/home/machine /home/machine nfs ...

   for each fileserver on the network.

   There are numerous problems with this system.  The mount list can
become quite large and some of the machines may be down when a system is
booted.  When a new fileserver is installed, `/etc/fstab' must be
updated on every machine, the mount directory created and the filesystem
mounted.

   In many environments most people use the same few workstations, but
it is convenient to go to a colleague's machine and access your own
files.  When a server goes down, it can cause a process on a client
machine to hang.  By minimizing the mounted filesystems to only include
those actively being used, there is less chance that a filesystem will
be mounted when a server goes down.

   The following is a short extract from a map taken from a research
fileserver at Imperial College.

   Note the entry for `localhost' which is used for users such as the
operator (`opr') who have a home directory on most machine as
`/home/localhost/opr'.

     /defaults       opts:=rw,intr,grpid,nosuid
     charm           host!=${key};type:=nfs;rhost:=${key};rfs:=/home/${key} \
                     host==${key};type:=ufs;dev:=/dev/xd0g
     #
     ...
     
     #
     localhost       type:=link;fs:=${host}
     ...
     #
     # dylan has two user disks so have a
     # top directory in which to mount them.
     #
     dylan           type:=auto;fs:=${map};pref:=${key}/
     #
     dylan/dk2       host!=dylan;type:=nfs;rhost:=dylan;rfs:=/home/${key} \
                     host==dylan;type:=ufs;dev:=/dev/dsk/2s0
     #
     dylan/dk5       host!=dylan;type:=nfs;rhost:=dylan;rfs:=/home/${key} \
                     host==dylan;type:=ufs;dev:=/dev/dsk/5s0
     ...
     #
     toytown         host!=${key};type:=nfs;rhost:=${key};rfs:=/home/${key} \
                     host==${key};type:=ufs;dev:=/dev/xy1g
     ...
     #
     zebedee         host!=${key};type:=nfs;rhost:=${key};rfs:=/home/${key} \
                     host==${key};type:=ufs;dev:=/dev/dsk/1s0
     #
     # Just for access...
     #
     gould           type:=auto;fs:=${map};pref:=${key}/
     gould/staff     host!=gould;type:=nfs;rhost:=gould;rfs:=/home/${key}
     #
     gummo           host!=${key};type:=nfs;rhost:=${key};rfs:=/home/${key}
     ...

   This map is shared by most of the machines listed so on those
systems any of the user disks is accessible via a consistent name.  Amd
is started with the following command

     amd /home amd.home

   Note that when mounting a remote filesystem, the "automounted" mount
point is referenced, so that the filesystem will be mounted if it is
not yet (at the time the remote `mountd' obtains the file handle).


File: am-utils.info,  Node: Home Directories,  Next: Architecture Sharing,  Prev: User Filesystems,  Up: Examples

Home Directories
================

   One convention for home directories is to locate them in `/homes' so
user `jsp''s home directory is `/homes/jsp'.  With more than a single
fileserver it is convenient to spread user files across several
machines.  All that is required is a mount-map which converts login
names to an automounted directory.

   Such a map might be started by the command:

     amd /homes amd.homes

   where the map `amd.homes' contained the entries:

     /defaults   type:=link   # All the entries are of type:=link
     jsp         fs:=/home/charm/jsp
     njw         fs:=/home/dylan/dk5/njw
     ...
     phjk        fs:=/home/toytown/ai/phjk
     sjv         fs:=/home/ganymede/sjv

   Whenever a login name is accessed in `/homes' a symbolic link
appears pointing to the real location of that user's home directory.  In
this example, `/homes/jsp' would appear to be a symbolic link pointing
to `/home/charm/jsp'.  Of course, `/home' would also be an automount
point.

   This system causes an extra level of symbolic links to be used.
Although that turns out to be relatively inexpensive, an alternative is
to directly mount the required filesystems in the `/homes' map.  The
required map is simple, but long, and its creation is best automated.
The entry for `jsp' could be:

     jsp   -sublink:=${key};rfs:=/home/charm \
                    host==charm;type:=ufs;dev:=/dev/xd0g \
                    host!=charm;type:=nfs;rhost:=charm

   This map can become quite big if it contains a large number of
entries.  By combining two other features of Amd it can be greatly
simplified.

   First the UFS partitions should be mounted under the control of
`/etc/fstab', taking care that they are mounted in the same place that
Amd would have automounted them.  In most cases this would be something
like `/a/"host"/home/"host"' and `/etc/fstab' on host `charm' would
have a line:

     /dev/xy0g /a/charm/home/charm 4.2 rw,nosuid,grpid 1 5

   The map can then be changed to:

     /defaults    type:=nfs;sublink:=${key};opts:=rw,intr,nosuid,grpid
     jsp          rhost:=charm;rfs:=/home/charm
     njw          rhost:=dylan;rfs:=/home/dylan/dk5
     ...
     phjk         rhost:=toytown;rfs:=/home/toytown;sublink:=ai/${key}
     sjv          rhost:=ganymede;rfs:=/home/ganymede

   This map operates as usual on a remote machine (ie `${host}' not
equal to `${rhost}').  On the machine where the filesystem is stored
(ie `${host}' equal to `${rhost}'), Amd will construct a local
filesystem mount point which corresponds to the name of the locally
mounted UFS partition.  If Amd is started with the `-r' option then
instead of attempting an NFS mount, Amd will simply inherit the UFS
mount (*note Inheritance Filesystem::).  If `-r' is not used then a
loopback NFS mount will be made.  This type of mount is known to cause
a deadlock on many systems.

